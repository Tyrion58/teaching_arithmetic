{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 这个notebook用来测试addition evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import GPTConfig, GPT\n",
    "import torch\n",
    "\n",
    "init_from = 'resume'\n",
    "\n",
    "\n",
    "if init_from == 'resume':\n",
    "    # init from a model saved in a specific directory\n",
    "    ckpt_path = './out/out-addition-label/ckpt_acc.pt'\n",
    "    checkpoint = torch.load(ckpt_path, map_location='cuda')\n",
    "    gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "    model = GPT(gptconf)\n",
    "    state_dict = checkpoint['model']\n",
    "    unwanted_prefix = '_orig_mod.'\n",
    "    for k,v in list(state_dict.items()):\n",
    "        if k.startswith(unwanted_prefix):\n",
    "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "    model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def get_encode_decode(meta_path=None, tokenizer='char'):\n",
    "    import pickle, tiktoken\n",
    "    # look for the meta pickle in case it is available in the dataset folder\n",
    "    load_meta = False\n",
    "    if meta_path and tokenizer == 'char':\n",
    "        print(f\"Loading meta from {meta_path}...\")\n",
    "        with open(meta_path, 'rb') as f:\n",
    "            meta = pickle.load(f)\n",
    "        # TODO want to make this more general to arbitrary encoder/decoder schemes\n",
    "        stoi, itos = meta['stoi'], meta['itos']\n",
    "        encode = lambda s: [stoi[c] for c in s]\n",
    "        decode = lambda l: ''.join([itos[i] for i in l])\n",
    "    elif tokenizer:\n",
    "        print(f\"Trying to load tiktoken's openAI {tokenizer} tokenizer\")\n",
    "        enc = tiktoken.get_encoding(f\"{tokenizer}\")\n",
    "        encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
    "        decode = lambda l: enc.decode(l)\n",
    "    else:\n",
    "        # ok let's assume gpt-2 encodings by default\n",
    "        print(\"No meta.pkl found, assuming GPT-2 encodings...\")\n",
    "        enc = tiktoken.get_encoding(\"gpt2\")\n",
    "        encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
    "        decode = lambda l: enc.decode(l)\n",
    "\n",
    "    return encode, decode\n",
    "\n",
    "def get_abc(expression: str):\n",
    "    \"\"\"\n",
    "    return: a(str), b(str), c(int), operation(str)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 尝试将表达式中的 'a' 和 'b' 转换为整数\n",
    "        if '+' in expression:\n",
    "            operation = '+'\n",
    "        [a, b] = expression.split(operation)\n",
    "        b = b[:-1]\n",
    "        if operation == '+':\n",
    "            # 计算和\n",
    "            c = int(a) + int(b)\n",
    "\n",
    "        # 返回结果\n",
    "        return a, b, c, '+'\n",
    "    except ValueError:\n",
    "        # 如果转换失败，抛出异常\n",
    "        raise ValueError(\"Invalid input. 'a' and 'b' must be integers.\")\n",
    "\n",
    "\n",
    "def get_num_digits(a: str):\n",
    "    if a == '':\n",
    "        return 0\n",
    "    else:\n",
    "        if '.' in a: # if a contains a decimal point\n",
    "            return len(a) - 1\n",
    "        else:\n",
    "            return len(str(int(a)))\n",
    "        \n",
    "        \n",
    "def numCarryOps(a, b, binary=False):\n",
    "    def digitSum(n):\n",
    "        return sum(map(int,str(n)))\n",
    "    if b == '':\n",
    "        return 0\n",
    "    \n",
    "    if not binary:\n",
    "        a,b=int(a),int(b)        \n",
    "        # assert(a >= 0); assert(b >= 0);\n",
    "        return int((digitSum(a) + digitSum(b) - digitSum(a+b)) / 9)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "        #c = int(a,2) + int(b,2)\n",
    "        #return int((digitSum(a) + digitSum(b) - digitSum(convert_to_binary(c))) )\n",
    "        \n",
    "def is_number(s):\n",
    "    # handle \"xey\" case (e.g. 1.2e-3) - we do not use this notation in our dataset\n",
    "    if 'e' in s:\n",
    "        return False\n",
    "    elif 'E' in s:\n",
    "        return False\n",
    "    elif 'inf' in s or \"INF\" in s:\n",
    "        return False\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_addition_batch(config, model, ctx, encode, decode, judge = False, num_digit=3):\n",
    "\n",
    "    model.eval()\n",
    "    start = config['start']\n",
    "    device = config['device']\n",
    "    print(device)\n",
    "    \n",
    "    test_batch_size = config['test_batch_size'] if 'test_batch_size' in config.keys() else 128\n",
    "    max_new_tokens = config['max_new_tokens'] if 'max_new_tokens' in config.keys() else num_digit+4\n",
    "    \n",
    "    temperature = config['temperature'] if 'temperature' in config.keys() else 0.2\n",
    "    top_k = config['top_k'] if 'top_k' in config.keys() else 200\n",
    "    \n",
    "    print(f'evaluating addition from: {start}')\n",
    "    \n",
    "    if start.startswith('FILE:'):\n",
    "        with open(start[5:], 'r', encoding='utf-8') as f:\n",
    "            # 除去每一行后面的空白字符，保存为列表，列表的每一个元素是一个算式，如“2+2=”\n",
    "            lines = [line.rstrip() for line in f]\n",
    "    \n",
    "    else:\n",
    "        raise NotImplementedError(\"This method is not implemented yet!\")\n",
    "    \n",
    "    correct = 0\n",
    "    pred_correct = 0\n",
    "    #总行数，也是总算式个数\n",
    "    total = len(lines)\n",
    "    \n",
    "    carry_dictionary={f'carry{i}_correct':0 for i in range(num_digit+1)}\n",
    "    #注意区别，corrtec和total\n",
    "    carry_dictionary.update({f'carry{i}_total':0 for i in range(num_digit+1)})\n",
    "    prompt_dict = {}\n",
    "    \n",
    "    for line_idx in tqdm(range(total)):\n",
    "        #line_idx是所取出算式的index，取出对应行line\n",
    "        line = lines[line_idx]\n",
    "        line.strip('\\n')\n",
    "        # 对line这个string做编码\n",
    "        start_ids = encode(line)\n",
    "        # 将编码转换为张量，并额外加一个维度，从len(start_ids)变为(1,len(start_ids))\n",
    "        x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "        # 在character level tokenization时，这个len_x其实就是len(start_ids)。。。\n",
    "        len_x = len(x[0])\n",
    "        a,b,c,op = get_abc(line)\n",
    "        a_d, b_d, num_carry = get_num_digits(a), get_num_digits(b), numCarryOps(a,b)\n",
    "        prompt_length = len(start_ids)\n",
    "        # NOTE: prompt_length != len(line) if we're not using character level tokenization\n",
    "        input_tuple = (x, len(line), line[0], a, b, c, a_d, b_d, num_carry)\n",
    "        if prompt_length in prompt_dict.keys():\n",
    "            prompt_dict[prompt_length].append(input_tuple)\n",
    "        else:\n",
    "            prompt_dict[prompt_length] = [input_tuple]\n",
    "        # prompt是一个字典，键值是所有可能出现的prompt_length\n",
    "        # 这样划分是为了保证每一个batch中的len_x相等\n",
    "        \n",
    "    # construct batches of prompts now\n",
    "    batch_list = []\n",
    "    for prompt_length in prompt_dict.keys():\n",
    "        input_tuple_list = prompt_dict[prompt_length]\n",
    "        for batch_idx in range(math.ceil(len(input_tuple_list)/test_batch_size)):\n",
    "            #每个sequence（或算式）对应一个tuple，每test_batch_size个tuple划分为同一个batch，对应这一个list，\n",
    "            # 也就是每个list就是一个batch，所有batch组成一个更大的batch_list\n",
    "            batch_list.append(input_tuple_list[batch_idx*test_batch_size:(batch_idx+1)*test_batch_size])\n",
    "                \n",
    "    for batch_idx in tqdm(range(len(batch_list))):\n",
    "        batch = batch_list[batch_idx]\n",
    "        # 单取出所有x\n",
    "        x_list = [input_tuple[0] for input_tuple in batch]\n",
    "        # x.size=(batch_size, )\n",
    "        x = torch.cat(x_list, dim=0)\n",
    "        # run generation\n",
    "        with torch.no_grad():\n",
    "            with ctx:\n",
    "                y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "                outcome_list = [decode(y_i.tolist()) for y_i in y]\n",
    "                # 下面逐个分析这个batch中的model的预测结果\n",
    "                for i, outcome in enumerate(outcome_list):\n",
    "                    Pred = None\n",
    "                    # 取出对应的tuple\n",
    "                    _, len_x, line_start, a, b, c, a_d, b_d, num_carry = batch[i]\n",
    "                    c_hat = outcome[len_x:]\n",
    "                    if '$' == line_start: # handle $ prompt $\n",
    "                        c_hat = c_hat.split('$')[0]\n",
    "                    else:\n",
    "                        c_hat = c_hat.split('\\n')[0]\n",
    "                        \n",
    "                        if 'T' == c_hat[-1] or 'F' == c_hat[-1]:\n",
    "                            Pred = c_hat[-1]\n",
    "                            c_hat = c_hat[:-1]\n",
    "                            \n",
    "                    c_hat2 = c_hat.strip()\n",
    "                    c_hat2 = c_hat2.split('\\n')[0]\n",
    "                    \n",
    "                    if is_number(c_hat2):\n",
    "                        if '.' in c_hat2:\n",
    "                            c_hat2 = float(c_hat2)\n",
    "                        else:\n",
    "                            c_hat2 = int(c_hat2)\n",
    "                    else: # c_hat2 is not a number\n",
    "                        c = str(c)\n",
    "                        \n",
    "                    if op in ['+','-','*']:\n",
    "                        if c == c_hat2:\n",
    "                            correct+=1\n",
    "                            carry_dictionary[f'carry{num_carry}_correct']+=1\n",
    "                            if Pred == 'T':\n",
    "                                pred_correct+=1\n",
    "                                \n",
    "                        elif Pred == 'F':\n",
    "                            pred_correct+=1\n",
    "                    else:\n",
    "                        raise NotImplementedError\n",
    "                    \n",
    "                    \n",
    "                    carry_dictionary[f'carry{num_carry}_total']+=1\n",
    "                    # metric_types = ['mse', 'normalized_mse', 'digit_wise_difference', 'incorrect_digit_count']\n",
    "    if judge:\n",
    "        pred_accuracy = pred_correct/total*100\n",
    "        print(f\"Judgement accuracy of {total} examples: {pred_correct}/{total} ({pred_accuracy}%)\")\n",
    "    accuracy = correct/total*100\n",
    "    print(f\"accuracy of {total} examples: {correct}/{total} ({accuracy}%)\")\n",
    "    accuracy_dictionary = {f'carry{i}': carry_dictionary[f'carry{i}_correct']/carry_dictionary[f'carry{i}_total']*100 \\\n",
    "        if carry_dictionary[f'carry{i}_total']!=0 else np.nan for i in range(num_digit+1)}\n",
    "    print(accuracy_dictionary)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    return accuracy, accuracy_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "config={\n",
    "    'start': 'FILE:./data/addition_label/prompt_addition_label_labeled10000.txt',\n",
    "    'device': 'cuda',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode, decode = get_encode_decode('./data/addition_label/meta.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import nullcontext\n",
    "\n",
    "dtype = 'bfloat16'\n",
    "device = 'cuda'\n",
    "model = model.to(device)\n",
    "device_type = 'cuda'\n",
    "# note: float16 data type will automatically use a GradScaler\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "eval_addition_batch(config, model, ctx, encode, decode, judge=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_samples(config, model, encode, decode, device, num_digit=3):\n",
    "    start = config['start']\n",
    "    \n",
    "    if start.startswith('FILE:'):\n",
    "        with open(start[5:], 'r', encoding='utf-8') as f:\n",
    "            # 除去每一行后面的空白字符，保存为列表，列表的每一个元素是一个算式，如“2+2=”\n",
    "            lines = [line.rstrip() for line in f]\n",
    "    \n",
    "    else:\n",
    "        raise NotImplementedError(\"This method is not implemented yet!\")\n",
    "    \n",
    "    total = 100\n",
    "    output = []\n",
    "    for line_idx in tqdm(range(total)):\n",
    "        #line_idx是所取出算式的index，取出对应行line\n",
    "        line = lines[line_idx]\n",
    "        line = line.split('\\n')[0]\n",
    "        # 对line这个string做编码\n",
    "        start_ids = encode(line)\n",
    "        # 将编码转换为张量，并额外加一个维度，从len(start_ids)变为(1,len(start_ids))\n",
    "        x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "        y = model.generate(x, max_new_tokens=num_digit+2, temperature=0.2)\n",
    "        outcome_list = [decode(y_i.tolist()) for y_i in y]\n",
    "        output.append(outcome_list[0])\n",
    "    \n",
    "    return output\n",
    "#out = give_samples(config, model, encode, decode, 'cuda')\n",
    "\n",
    "#outpath = 'samples.txt'\n",
    "#with open(outpath, 'w') as f:\n",
    "#    for instance in out:\n",
    "#        exp = instance.strip()\n",
    "#        f.write(exp+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 10.63M\n"
     ]
    }
   ],
   "source": [
    "from model import GPTConfig, GPT\n",
    "import torch\n",
    "\n",
    "init_from = 'resume'\n",
    "\n",
    "\n",
    "if init_from == 'resume':\n",
    "    # init from a model saved in a specific directory\n",
    "    ckpt_path = './out/out-addition-labelV2/ckpt_acc.pt'\n",
    "    checkpoint = torch.load(ckpt_path, map_location='cuda')\n",
    "    gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "    model = GPT(gptconf)\n",
    "    state_dict = checkpoint['model']\n",
    "    unwanted_prefix = '_orig_mod.'\n",
    "    for k,v in list(state_dict.items()):\n",
    "        if k.startswith(unwanted_prefix):\n",
    "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "    model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "config={\n",
    "    'start': 'FILE:./data/addition_labelV2/prompt_3digit_V210000.txt',\n",
    "    'device': 'cuda',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading meta from ./data/addition_labelV2/meta.pkl...\n"
     ]
    }
   ],
   "source": [
    "encode, decode = get_encode_decode('./data/addition_labelV2/meta.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating addition from: FILE:./data/addition_labelV2/prompt_3digit_V210000.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:07<00:00, 1251.19it/s]\n",
      "100%|██████████| 80/80 [00:04<00:00, 19.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judgement accuracy of 10000 examples: 9429/10000 (94.28999999999999%)\n",
      "accuracy of 10000 examples: 9429/10000 (94.28999999999999%)\n",
      "{'carry0': 94.25219941348973, 'carry1': 92.57330775554946, 'carry2': 95.13888888888889, 'carry3': 96.92653673163419}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(94.28999999999999,\n",
       " 94.28999999999999,\n",
       " {'carry0': 94.25219941348973,\n",
       "  'carry1': 92.57330775554946,\n",
       "  'carry2': 95.13888888888889,\n",
       "  'carry3': 96.92653673163419})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from contextlib import nullcontext\n",
    "\n",
    "dtype = 'bfloat16'\n",
    "device = 'cuda'\n",
    "model = model.to(device)\n",
    "device_type = 'cuda'\n",
    "# note: float16 data type will automatically use a GradScaler\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "eval_addition_batch(config, model, ctx, encode, decode, judge=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:04<00:00, 20.25it/s]\n"
     ]
    }
   ],
   "source": [
    "out = give_samples(config, model, encode, decode, 'cuda')\n",
    "\n",
    "outpath = 'samples.txt'\n",
    "with open(outpath, 'w') as f:\n",
    "    for instance in out:\n",
    "        exp = instance.strip()\n",
    "        exp = exp.split('\\n')[0]\n",
    "        f.write(exp+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
