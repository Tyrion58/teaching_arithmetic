{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试addition和judgement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main_utils import *\n",
    "mydevice = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主要使用了3种测试prompt\n",
    "- 为了检测model的计算能力，所有input以`T`开头，即我们只要求输出正确的计算结果，但同时我们会让模型多输出一位（判断结果是否正确），形如: `T1+2=`\n",
    "- 为了检测model在extra number型错误上的判断能力，按照训练时的方法制作negative instances。只要求模型输出一位，prompt形如：`1+2=3`和`2+3=58`\n",
    "- 为了检测model在其它错误上的判断能力，制作negative instances时以均匀分布对正确结果的其中一位采样，在该位上加1～9的随机数。只要求模型输出一位"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "\n",
    "class create_test_data:\n",
    "    def __init__(self, non_overlap_data_path, num_test_samples=100) -> None:\n",
    "        if not os.path.exists(non_overlap_data_path):\n",
    "            raise ValueError(\"There is no nonoverlap data file\")\n",
    "        self.non_overlap_data_path = non_overlap_data_path\n",
    "        self.num_test_samples = num_test_samples\n",
    "        self.samples = None\n",
    "        # Open non-overlap data path to get a bunch of test samples\n",
    "        with open(self.non_overlap_data_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            random.shuffle(lines)\n",
    "            self.samples = lines[:self.num_test_samples]\n",
    "            \n",
    "        \n",
    "    def create_prompt(self):\n",
    "        \"\"\"\n",
    "        To create prompt data file like: 'Ta+b=' .\n",
    "        The output should be correct answer and judgement\n",
    "        \"\"\"\n",
    "        with open(f'prompt.txt', 'w') as f2:\n",
    "            for line in self.samples:\n",
    "                prompt = line.split('=')[0]+'=\\n'\n",
    "                f2.write(prompt)\n",
    "                    \n",
    "    def create_add_noise_judge_prompt(self):\n",
    "        \"\"\"\n",
    "        To create prompt data file like: 'a+b=c' and 'a+b=d', where 'd' means wrong answer\n",
    "        The output should be judgement\n",
    "        \"\"\"\n",
    "        with open('add_noise_judge_prompt.txt', 'w') as f3:\n",
    "            for line in self.samples:\n",
    "                # 取出表达式部分\n",
    "                prompt = line.split('T')[1]\n",
    "                # prompt = prompt.split('=')[0].strip()\n",
    "                new_prompt = self.modify_result(prompt, random.randint(1, 9), 'noise_add')\n",
    "                f3.write(new_prompt + '\\n')\n",
    "                \n",
    "    def create_extra_num_judge_prompt(self):\n",
    "        \"\"\"\n",
    "        To create prompt data file like: 'a+b=c' and 'a+b=cd', where 'd' means extra number\n",
    "        The output should be judgement\n",
    "        \"\"\"\n",
    "        with open('extra_num_judge_prompt.txt', 'w') as f4:\n",
    "            for line in self.samples:\n",
    "                # 取出表达式部分\n",
    "                prompt = line.split('T')[1]\n",
    "                # prompt = prompt.split('=')[0].strip()\n",
    "                new_prompt = self.modify_result(prompt, random.randint(1, 9), 'extra_num')\n",
    "                f4.write(new_prompt + '\\n')\n",
    "\n",
    "    \n",
    "    def modify_result(self, expression, addend, mode):\n",
    "        # 使用正则表达式提取表达式中的数字\n",
    "        match = re.match(r'(\\d+)\\+(\\d+)=(\\d+)', expression)\n",
    "    \n",
    "        if match:\n",
    "            # 提取数字并计算新的结果\n",
    "            num1 = int(match.group(1))\n",
    "            num2 = int(match.group(2))\n",
    "            result = int(match.group(3))\n",
    "            num_digit = len(match.group(3).strip())\n",
    "            if mode == 'extra_num':\n",
    "                if random.uniform(0,1)>0.5 and random.uniform(0,1)<0.75:\n",
    "                    extra = random.randint(1, 9)\n",
    "                    new_expression = f\"F{num1}+{num2}={result}{extra}\"\n",
    "                elif random.uniform(0,1)>0.75:\n",
    "                    extra = random.randint(1, 9)\n",
    "                    new_expression = f\"F{num1}+{num2}={extra}{result}\"\n",
    "                else:\n",
    "                    new_expression = f\"T{num1}+{num2}={result}\"\n",
    "            elif mode == 'noise_add':\n",
    "                if random.uniform(0,1)>0.5:\n",
    "                    # 决定添加错误的位置\n",
    "                    wrong_loc = random.randint(0, num_digit)\n",
    "        \n",
    "                    new_result = result + addend * (10**wrong_loc)\n",
    "\n",
    "                    # 构建新的表达式\n",
    "                    new_expression = f\"F{num1}+{num2}={new_result}\"\n",
    "                else:\n",
    "                    new_expression = f\"T{num1}+{num2}={result}\"\n",
    "            \n",
    "            else:\n",
    "                return \"Invalid modify pattern\"\n",
    "        \n",
    "            return new_expression\n",
    "        else:\n",
    "            return \"Invalid expression format\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_overlap_data_path = './data/get_data_with_label/train_3digit_bilabeled10000_nonoverlap.txt'\n",
    "num_examples = 10000\n",
    "creator = create_test_data(non_overlap_data_path, num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "creator.create_prompt()\n",
    "creator.create_add_noise_judge_prompt()\n",
    "creator.create_extra_num_judge_prompt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 10.63M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import GPTConfig, GPT\n",
    "import torch\n",
    "\n",
    "# init from a model saved in a specific directory\n",
    "ckpt_path = 'bilabel_ckpt_acc.pt'\n",
    "checkpoint = torch.load(ckpt_path, map_location=mydevice)\n",
    "gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "model = GPT(gptconf)\n",
    "state_dict = checkpoint['model']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "config={\n",
    "    'start': 'FILE:./data/addition_bilabel/prompt_3digit_10000.txt',\n",
    "    'device': mydevice,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading meta from ./data/addition_bilabel/meta.pkl...\n"
     ]
    }
   ],
   "source": [
    "encode, decode = get_encode_decode('./data/addition_bilabel/meta.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'213+199=412T'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = '213+199=412'\n",
    "ids = encode(x)\n",
    "input = (torch.tensor(ids, dtype=torch.long, device=mydevice)[None, ...])\n",
    "model.to(device=mydevice)\n",
    "output = model.generate(input, max_new_tokens=1)\n",
    "decode(output[0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 批量测试\n",
    "\n",
    "从txt文件中读取prompt进行测试，主要分为两种测试：\n",
    "\n",
    "- 正确性测试：只关注计算结果是否正确\n",
    "- 判断测试：只关注判断结果是否正确"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取prompt.txt，测试正确率与判断正确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating addition from: FILE:./prompt.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:02<00:00, 4554.07it/s]\n",
      "100%|██████████| 81/81 [00:19<00:00,  4.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judgement accuracy of 10000 examples: 9271/10000 (92.71000000000001%)\n",
      "accuracy of 10000 examples: 9267/10000 (92.67%)\n",
      "{'carry0': 90.9090909090909, 'carry1': 89.44240022643646, 'carry2': 95.0613676212741, 'carry3': 97.15950473415877}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(92.71000000000001,\n",
       " 92.67,\n",
       " {'carry0': 90.9090909090909,\n",
       "  'carry1': 89.44240022643646,\n",
       "  'carry2': 95.0613676212741,\n",
       "  'carry3': 97.15950473415877})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from contextlib import nullcontext\n",
    "ctx = nullcontext()\n",
    "config={\n",
    "    'start': 'FILE:./prompt.txt',\n",
    "    'device': mydevice,\n",
    "    'temperature': 0.8\n",
    "}\n",
    "eval_addition_batch(config, model, ctx, encode, decode, judge=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取extra_num_judge_prompt.txt，测试判断能力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating addition from: FILE:./extra_num_judge_prompt.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:02<00:00, 4552.71it/s]\n",
      "100%|██████████| 82/82 [00:07<00:00, 11.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judgement accuracy of 10000 examples: 9231/10000 (92.31%)\n",
      "No judging probability of 10000 examples: 650/10000 (6.5%)\n",
      "{'carry0': 89.11483253588517, 'carry1': 91.14067364845741, 'carry2': 93.36645236703683, 'carry3': 96.5768390386016}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(92.31,\n",
       " 6.5,\n",
       " {'carry0': 89.11483253588517,\n",
       "  'carry1': 91.14067364845741,\n",
       "  'carry2': 93.36645236703683,\n",
       "  'carry3': 96.5768390386016})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx = nullcontext()\n",
    "config={\n",
    "    'start': 'FILE:./extra_num_judge_prompt.txt',\n",
    "    'device': mydevice,\n",
    "}\n",
    "eval_judge_batch(config, model, ctx, encode, decode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取add_noise_judge_prompt.txt，测试模型判断能力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating addition from: FILE:./add_noise_judge_prompt.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:02<00:00, 4224.60it/s]\n",
      "100%|██████████| 82/82 [00:06<00:00, 12.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judgement accuracy of 10000 examples: 6094/10000 (60.940000000000005%)\n",
      "No judging probability of 10000 examples: 1182/10000 (11.82%)\n",
      "{'carry0': 53.88755980861244, 'carry1': 59.835833569204645, 'carry2': 62.85797779076563, 'carry3': 67.5892206846322}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(60.940000000000005,\n",
       " 11.82,\n",
       " {'carry0': 53.88755980861244,\n",
       "  'carry1': 59.835833569204645,\n",
       "  'carry2': 62.85797779076563,\n",
       "  'carry3': 67.5892206846322})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx = nullcontext()\n",
    "config={\n",
    "    'start': 'FILE:./add_noise_judge_prompt.txt',\n",
    "    'device': mydevice,\n",
    "}\n",
    "eval_judge_batch(config, model, ctx, encode, decode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由此可见，\n",
    "- 不需要进行预训练+微调，模型已经有判断正误的能力\n",
    "- 训练集中negative instances的设置方式会对模型的判断能力产生较大影响"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 同时使用两类negative instances训练\n",
    "\n",
    "首先创建测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_overlap_data_path = './data/get_data_with_label/train_3digit_bilabeled10000_nonoverlap_new.txt'\n",
    "num_examples = 10000\n",
    "new_creator = create_test_data(non_overlap_data_path, num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_creator.create_prompt()\n",
    "new_creator.create_extra_num_judge_prompt()\n",
    "new_creator.create_add_noise_judge_prompt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 10.63M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(15, 384)\n",
       "    (wpe): Embedding(256, 384)\n",
       "    (drop): Dropout(p=0.2, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=384, out_features=1152, bias=False)\n",
       "          (c_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.2, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=384, out_features=1536, bias=False)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=1536, out_features=384, bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=384, out_features=15, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import GPTConfig, GPT\n",
    "import torch\n",
    "\n",
    "# init from a model saved in a specific directory\n",
    "ckpt_path = 'new_bilabel_ckpt_acc.pt'\n",
    "checkpoint = torch.load(ckpt_path, map_location=mydevice)\n",
    "gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "model = GPT(gptconf)\n",
    "state_dict = checkpoint['model']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(mydevice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading meta from ./data/addition_bilabel/meta.pkl...\n"
     ]
    }
   ],
   "source": [
    "encode, decode = get_encode_decode('./data/addition_bilabel/meta.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating addition from: FILE:./prompt.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:02<00:00, 4252.95it/s]\n",
      "100%|██████████| 81/81 [00:20<00:00,  3.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judgement accuracy of 10000 examples: 8378/10000 (83.78%)\n",
      "accuracy of 10000 examples: 9389/10000 (93.89%)\n",
      "{'carry0': 93.84615384615384, 'carry1': 92.77440706012135, 'carry2': 94.23476968796433, 'carry3': 96.13343442001516}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(83.78,\n",
       " 93.89,\n",
       " {'carry0': 93.84615384615384,\n",
       "  'carry1': 92.77440706012135,\n",
       "  'carry2': 94.23476968796433,\n",
       "  'carry3': 96.13343442001516})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from contextlib import nullcontext\n",
    "ctx = nullcontext()\n",
    "config={\n",
    "    'start': 'FILE:./prompt.txt',\n",
    "    'device': mydevice,\n",
    "    'temperature': 0.8\n",
    "}\n",
    "eval_addition_batch(config, model, ctx, encode, decode, judge=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试extra number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating addition from: FILE:./extra_num_judge_prompt.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:02<00:00, 4511.77it/s]\n",
      "100%|██████████| 84/84 [00:06<00:00, 12.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judgement accuracy of 10000 examples: 0/10000 (0.0%)\n",
      "No judging probability of 10000 examples: 10000/10000 (100.0%)\n",
      "{'carry0': 0.0, 'carry1': 0.0, 'carry2': 0.0, 'carry3': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 100.0, {'carry0': 0.0, 'carry1': 0.0, 'carry2': 0.0, 'carry3': 0.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx = nullcontext()\n",
    "config={\n",
    "    'start': 'FILE:./extra_num_judge_prompt.txt',\n",
    "    'device': mydevice,\n",
    "}\n",
    "eval_judge_batch(config, model, ctx, encode, decode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试add noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating addition from: FILE:./add_noise_judge_prompt.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:02<00:00, 4279.48it/s]\n",
      "100%|██████████| 82/82 [00:07<00:00, 11.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judgement accuracy of 10000 examples: 0/10000 (0.0%)\n",
      "No judging probability of 10000 examples: 10000/10000 (100.0%)\n",
      "{'carry0': 0.0, 'carry1': 0.0, 'carry2': 0.0, 'carry3': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 100.0, {'carry0': 0.0, 'carry1': 0.0, 'carry2': 0.0, 'carry3': 0.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx = nullcontext()\n",
    "config={\n",
    "    'start': 'FILE:./add_noise_judge_prompt.txt',\n",
    "    'device': mydevice,\n",
    "}\n",
    "eval_judge_batch(config, model, ctx, encode, decode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型更本没有做判断，这是因为同时存在extra number和add noise两种错误"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 同时使用两类negative instances训练, 0.8p和0.2n, space\n",
    "\n",
    "首先创建测试数据\n",
    "发现0.7p+0.3n表现过差，但是在改为0.8p+0.2n后得到了目前为止最好的表现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "\n",
    "class create_test_data:\n",
    "    def __init__(self, non_overlap_data_path, num_test_samples=100) -> None:\n",
    "        if not os.path.exists(non_overlap_data_path):\n",
    "            raise ValueError(\"There is no nonoverlap data file\")\n",
    "        self.non_overlap_data_path = non_overlap_data_path\n",
    "        self.num_test_samples = num_test_samples\n",
    "        self.samples = None\n",
    "        # Open non-overlap data path to get a bunch of test samples\n",
    "        with open(self.non_overlap_data_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            random.shuffle(lines)\n",
    "            self.samples = lines[:self.num_test_samples]\n",
    "            \n",
    "        \n",
    "    def create_prompt(self):\n",
    "        \"\"\"\n",
    "        To create prompt data file like: 'Ta+b=' .\n",
    "        The output should be correct answer and judgement\n",
    "        \"\"\"\n",
    "        with open(f'prompt.txt', 'w') as f2:\n",
    "            for line in self.samples:\n",
    "                prompt = line.split('=')[0]+'=\\n'\n",
    "                f2.write(prompt)\n",
    "                    \n",
    "    def create_add_noise_judge_prompt(self):\n",
    "        \"\"\"\n",
    "        To create prompt data file like: 'a+b=c' and 'a+b=d', where 'd' means wrong answer\n",
    "        The output should be judgement\n",
    "        \"\"\"\n",
    "        with open('add_noise_judge_prompt.txt', 'w') as f3:\n",
    "            for line in self.samples:\n",
    "                # 取出表达式部分\n",
    "                prompt = line.split('T')[1].strip()\n",
    "                # prompt = prompt.split('=')[0].strip()\n",
    "                new_prompt = self.modify_result(prompt, random.randint(1, 9), 'noise_add')\n",
    "                f3.write(new_prompt + '\\n')\n",
    "                \n",
    "    def create_extra_num_judge_prompt(self):\n",
    "        \"\"\"\n",
    "        To create prompt data file like: 'a+b=c' and 'a+b=cd', where 'd' means extra number\n",
    "        The output should be judgement\n",
    "        \"\"\"\n",
    "        with open('extra_num_judge_prompt.txt', 'w') as f4:\n",
    "            for line in self.samples:\n",
    "                # 取出表达式部分\n",
    "                prompt = line.split('T')[1].strip()\n",
    "                # prompt = prompt.split('=')[0].strip()\n",
    "                new_prompt = self.modify_result(prompt, random.randint(1, 9), 'extra_num')\n",
    "                f4.write(new_prompt + '\\n')\n",
    "\n",
    "    \n",
    "    def modify_result(self, expression, addend, mode):\n",
    "        # 使用正则表达式提取表达式中的数字\n",
    "        match = re.match(r'(\\d+)\\+(\\d+)=(\\d+)', expression)\n",
    "    \n",
    "        if match:\n",
    "            # 提取数字并计算新的结果\n",
    "            num1 = int(match.group(1))\n",
    "            num2 = int(match.group(2))\n",
    "            result = int(match.group(3))\n",
    "            num_digit = len(match.group(3).strip())\n",
    "            if mode == 'extra_num':\n",
    "                if random.uniform(0,1)>0.5 and random.uniform(0,1)<0.75:\n",
    "                    extra = random.randint(1, 9)\n",
    "                    new_expression = f\"F{num1}+{num2}={result}{extra}\"\n",
    "                elif random.uniform(0,1)>0.75:\n",
    "                    extra = random.randint(1, 9)\n",
    "                    new_expression = f\"F{num1}+{num2}={extra}{result}\"\n",
    "                else:\n",
    "                    new_expression = f\"T{num1}+{num2}={result}\"\n",
    "            elif mode == 'noise_add':\n",
    "                if random.uniform(0,1)>0.5:\n",
    "                    # 决定添加错误的位置\n",
    "                    wrong_loc = random.randint(0, num_digit)\n",
    "        \n",
    "                    new_result = result + addend * (10**wrong_loc)\n",
    "\n",
    "                    # 构建新的表达式\n",
    "                    new_expression = f\"F{num1}+{num2}={new_result}\"\n",
    "                else:\n",
    "                    new_expression = f\"T{num1}+{num2}={result}\"\n",
    "            \n",
    "            else:\n",
    "                return \"Invalid modify pattern\"\n",
    "        \n",
    "            return new_expression\n",
    "        else:\n",
    "            return \"Invalid expression format\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_overlap_data_path = './data/get_data_with_label/train_3digit_bilabeled10000_nonoverlap_new_sp.txt'\n",
    "num_examples = 10000\n",
    "new_creator = create_test_data(non_overlap_data_path, num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_creator.create_prompt()\n",
    "new_creator.create_extra_num_judge_prompt()\n",
    "new_creator.create_add_noise_judge_prompt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 10.63M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(16, 384)\n",
       "    (wpe): Embedding(256, 384)\n",
       "    (drop): Dropout(p=0.2, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=384, out_features=1152, bias=False)\n",
       "          (c_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.2, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=384, out_features=1536, bias=False)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=1536, out_features=384, bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=384, out_features=16, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import GPTConfig, GPT\n",
    "import torch\n",
    "\n",
    "# init from a model saved in a specific directory\n",
    "ckpt_path = 'bilabel_ckpt_acc_sp.pt'\n",
    "checkpoint = torch.load(ckpt_path, map_location=mydevice)\n",
    "gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "model = GPT(gptconf)\n",
    "state_dict = checkpoint['model']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(mydevice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading meta from ./data/addition_bilabel_sp/meta.pkl...\n"
     ]
    }
   ],
   "source": [
    "encode, decode = get_encode_decode('./data/addition_bilabel_sp/meta.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating addition from: FILE:./prompt.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:02<00:00, 4454.05it/s]\n",
      "100%|██████████| 81/81 [00:21<00:00,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judgement accuracy of 10000 examples: 9454/10000 (94.54%)\n",
      "accuracy of 10000 examples: 9457/10000 (94.57%)\n",
      "{'carry0': 93.62354383813611, 'carry1': 92.82136894824707, 'carry2': 95.77960140679953, 'carry3': 97.28539985326485}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(94.54,\n",
       " 94.57,\n",
       " {'carry0': 93.62354383813611,\n",
       "  'carry1': 92.82136894824707,\n",
       "  'carry2': 95.77960140679953,\n",
       "  'carry3': 97.28539985326485})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from contextlib import nullcontext\n",
    "ctx = nullcontext()\n",
    "config={\n",
    "    'start': 'FILE:./prompt.txt',\n",
    "    'device': mydevice,\n",
    "    'temperature': 0.8\n",
    "}\n",
    "eval_addition_batch(config, model, ctx, encode, decode, judge=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试extra number的judgement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating addition from: FILE:./extra_num_judge_prompt.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:02<00:00, 3971.68it/s]\n",
      "100%|██████████| 83/83 [00:11<00:00,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judgement accuracy of 10000 examples: 9386/10000 (93.86%)\n",
      "No judging probability of 10000 examples: 243/10000 (2.4299999999999997%)\n",
      "{'carry0': 93.17617866004963, 'carry1': 94.05286343612335, 'carry2': 93.48079161816065, 'carry3': 95.15151515151516}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(93.86,\n",
       " 2.4299999999999997,\n",
       " {'carry0': 93.17617866004963,\n",
       "  'carry1': 94.05286343612335,\n",
       "  'carry2': 93.48079161816065,\n",
       "  'carry3': 95.15151515151516})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from contextlib import nullcontext\n",
    "ctx = nullcontext()\n",
    "config={\n",
    "    'start': 'FILE:./extra_num_judge_prompt.txt',\n",
    "    'device': mydevice,\n",
    "}\n",
    "eval_judge_batch(config, model, ctx, encode, decode, max_new_tokens=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试add digit noise的judgement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating addition from: FILE:./add_noise_judge_prompt.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:02<00:00, 3948.66it/s]\n",
      "100%|██████████| 82/82 [00:11<00:00,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judgement accuracy of 10000 examples: 6498/10000 (64.98%)\n",
      "No judging probability of 10000 examples: 432/10000 (4.32%)\n",
      "{'carry0': 59.42928039702233, 'carry1': 63.298458149779734, 'carry2': 67.05471478463329, 'carry3': 70.98484848484848}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(64.98,\n",
       " 4.32,\n",
       " {'carry0': 59.42928039702233,\n",
       "  'carry1': 63.298458149779734,\n",
       "  'carry2': 67.05471478463329,\n",
       "  'carry3': 70.98484848484848})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx = nullcontext()\n",
    "config={\n",
    "    'start': 'FILE:./add_noise_judge_prompt.txt',\n",
    "    'device': mydevice,\n",
    "}\n",
    "eval_judge_batch(config, model, ctx, encode, decode, max_new_tokens=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用新数据\n",
    "生成10000个训练样本，其中40%的样本除了正确的结果与label还包括两类负样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "\n",
    "class create_test_data:\n",
    "    def __init__(self, non_overlap_data_path, num_test_samples=100) -> None:\n",
    "        if not os.path.exists(non_overlap_data_path):\n",
    "            raise ValueError(\"There is no nonoverlap data file\")\n",
    "        self.non_overlap_data_path = non_overlap_data_path\n",
    "        self.num_test_samples = num_test_samples\n",
    "        self.samples = None\n",
    "        # Open non-overlap data path to get a bunch of test samples\n",
    "        with open(self.non_overlap_data_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            random.shuffle(lines)\n",
    "            self.samples = lines[:self.num_test_samples]\n",
    "            \n",
    "        \n",
    "    def create_prompt(self):\n",
    "        \"\"\"\n",
    "        To create prompt data file like: 'Ta+b=' .\n",
    "        The output should be correct answer and judgement\n",
    "        \"\"\"\n",
    "        with open(f'prompt.txt', 'w') as f2:\n",
    "            for line in self.samples:\n",
    "                prompt = line.split('=')[0]+'=\\n'\n",
    "                f2.write(prompt)\n",
    "                    \n",
    "    def create_add_noise_judge_prompt(self):\n",
    "        \"\"\"\n",
    "        To create prompt data file like: 'a+b=c' and 'a+b=d', where 'd' means wrong answer\n",
    "        The output should be judgement\n",
    "        \"\"\"\n",
    "        with open('add_noise_judge_prompt.txt', 'w') as f3:\n",
    "            for line in self.samples:\n",
    "                # 取出表达式部分\n",
    "                prompt = line.split('T')[1].strip()\n",
    "                # prompt = prompt.split('=')[0].strip()\n",
    "                new_prompt = self.modify_result(prompt, random.randint(1, 9), 'noise_add')\n",
    "                f3.write(new_prompt + '?\\n')\n",
    "                \n",
    "    def create_extra_num_judge_prompt(self):\n",
    "        \"\"\"\n",
    "        To create prompt data file like: 'a+b=c' and 'a+b=cd', where 'd' means extra number\n",
    "        The output should be judgement\n",
    "        \"\"\"\n",
    "        with open('extra_num_judge_prompt.txt', 'w') as f4:\n",
    "            for line in self.samples:\n",
    "                # 取出表达式部分\n",
    "                prompt = line.split('T')[1].strip()\n",
    "                # prompt = prompt.split('=')[0].strip()\n",
    "                new_prompt = self.modify_result(prompt, random.randint(1, 9), 'extra_num')\n",
    "                f4.write(new_prompt + '?\\n')\n",
    "\n",
    "    \n",
    "    def modify_result(self, expression, addend, mode):\n",
    "        # 使用正则表达式提取表达式中的数字\n",
    "        match = re.match(r'(\\d+)\\+(\\d+)=(\\d+)', expression)\n",
    "    \n",
    "        if match:\n",
    "            # 提取数字并计算新的结果\n",
    "            num1 = int(match.group(1))\n",
    "            num2 = int(match.group(2))\n",
    "            result = int(match.group(3))\n",
    "            num_digit = len(match.group(3).strip())\n",
    "            if mode == 'extra_num':\n",
    "                if random.uniform(0,1)>0.5 and random.uniform(0,1)<0.75:\n",
    "                    extra = random.randint(1, 9)\n",
    "                    new_expression = f\"F{num1}+{num2}={result}{extra}\"\n",
    "                elif random.uniform(0,1)>0.75:\n",
    "                    extra = random.randint(1, 9)\n",
    "                    new_expression = f\"F{num1}+{num2}={extra}{result}\"\n",
    "                else:\n",
    "                    new_expression = f\"T{num1}+{num2}={result}\"\n",
    "            elif mode == 'noise_add':\n",
    "                if random.uniform(0,1)>0.5:\n",
    "                    # 决定添加错误的位置\n",
    "                    wrong_loc = random.randint(0, num_digit)\n",
    "        \n",
    "                    new_result = result + addend * (10**wrong_loc)\n",
    "\n",
    "                    # 构建新的表达式\n",
    "                    new_expression = f\"F{num1}+{num2}={new_result}\"\n",
    "                else:\n",
    "                    new_expression = f\"T{num1}+{num2}={result}\"\n",
    "            \n",
    "            else:\n",
    "                return \"Invalid modify pattern\"\n",
    "        \n",
    "            return new_expression\n",
    "        else:\n",
    "            return \"Invalid expression format\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_overlap_data_path = './data/get_data_with_label/train_3digit_bilabeled10000_nonoverlap.txt'\n",
    "num_examples = 10000\n",
    "new_creator = create_test_data(non_overlap_data_path, num_examples)\n",
    "new_creator.create_prompt()\n",
    "new_creator.create_extra_num_judge_prompt()\n",
    "new_creator.create_add_noise_judge_prompt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 10.63M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(16, 384)\n",
       "    (wpe): Embedding(256, 384)\n",
       "    (drop): Dropout(p=0.2, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=384, out_features=1152, bias=False)\n",
       "          (c_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.2, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=384, out_features=1536, bias=False)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=1536, out_features=384, bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=384, out_features=16, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import GPTConfig, GPT\n",
    "import torch\n",
    "\n",
    "# init from a model saved in a specific directory\n",
    "ckpt_path = 'ckpt_acc_bilabel(0.6p).pt'\n",
    "checkpoint = torch.load(ckpt_path, map_location=mydevice)\n",
    "gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "model = GPT(gptconf)\n",
    "state_dict = checkpoint['model']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(mydevice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading meta from ./data/addition_bilabel/meta.pkl...\n"
     ]
    }
   ],
   "source": [
    "encode, decode = get_encode_decode('./data/addition_bilabel/meta.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating addition from: FILE:./prompt.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 12625.53it/s]\n",
      "100%|██████████| 81/81 [00:01<00:00, 44.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judgement accuracy of 10000 examples: 9476/10000 (94.76%)\n",
      "accuracy of 10000 examples: 9478/10000 (94.78%)\n",
      "{'carry0': 95.66787003610109, 'carry1': 93.37062937062936, 'carry2': 95.57089444923785, 'carry3': 95.41213063763608}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(94.76,\n",
       " 94.78,\n",
       " {'carry0': 95.66787003610109,\n",
       "  'carry1': 93.37062937062936,\n",
       "  'carry2': 95.57089444923785,\n",
       "  'carry3': 95.41213063763608})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from contextlib import nullcontext\n",
    "ctx = nullcontext()\n",
    "config={\n",
    "    'start': 'FILE:./prompt.txt',\n",
    "    'device': mydevice,\n",
    "    'temperature': 0.8\n",
    "}\n",
    "eval_addition_batch(config, model, ctx, encode, decode, judge=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试extra number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating addition from: FILE:./extra_num_judge_prompt.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 13763.47it/s]\n",
      "100%|██████████| 83/83 [00:00<00:00, 152.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judgement accuracy of 10000 examples: 9371/10000 (93.71000000000001%)\n",
      "No judging probability of 10000 examples: 0/10000 (0.0%)\n",
      "{'carry0': 92.35860409145607, 'carry1': 93.2027972027972, 'carry2': 94.36295657175727, 'carry3': 95.10108864696734}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(93.71000000000001,\n",
       " 0.0,\n",
       " {'carry0': 92.35860409145607,\n",
       "  'carry1': 93.2027972027972,\n",
       "  'carry2': 94.36295657175727,\n",
       "  'carry3': 95.10108864696734})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from contextlib import nullcontext\n",
    "ctx = nullcontext()\n",
    "config={\n",
    "    'start': 'FILE:./extra_num_judge_prompt.txt',\n",
    "    'device': mydevice,\n",
    "}\n",
    "eval_judge_batch(config, model, ctx, encode, decode, max_new_tokens=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试add noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating addition from: FILE:./add_noise_judge_prompt.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 11979.32it/s]\n",
      "100%|██████████| 82/82 [00:00<00:00, 145.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judgement accuracy of 10000 examples: 6717/10000 (67.17%)\n",
      "No judging probability of 10000 examples: 0/10000 (0.0%)\n",
      "{'carry0': 62.15403128760529, 'carry1': 65.65034965034965, 'carry2': 68.99626114466494, 'carry3': 72.93934681181959}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(67.17,\n",
       " 0.0,\n",
       " {'carry0': 62.15403128760529,\n",
       "  'carry1': 65.65034965034965,\n",
       "  'carry2': 68.99626114466494,\n",
       "  'carry3': 72.93934681181959})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx = nullcontext()\n",
    "config={\n",
    "    'start': 'FILE:./add_noise_judge_prompt.txt',\n",
    "    'device': mydevice,\n",
    "}\n",
    "eval_judge_batch(config, model, ctx, encode, decode, max_new_tokens=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用best judgement acc model测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 10.63M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(16, 384)\n",
       "    (wpe): Embedding(256, 384)\n",
       "    (drop): Dropout(p=0.2, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=384, out_features=1152, bias=False)\n",
       "          (c_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.2, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=384, out_features=1536, bias=False)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=1536, out_features=384, bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=384, out_features=16, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import GPTConfig, GPT\n",
    "import torch\n",
    "\n",
    "# init from a model saved in a specific directory\n",
    "ckpt_path = 'ckpt_judge_acc_bilabel(0.6p).pt'\n",
    "checkpoint = torch.load(ckpt_path, map_location=mydevice)\n",
    "gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "model = GPT(gptconf)\n",
    "state_dict = checkpoint['model']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(mydevice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading meta from ./data/addition_bilabel/meta.pkl...\n"
     ]
    }
   ],
   "source": [
    "encode, decode = get_encode_decode('./data/addition_bilabel/meta.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating addition from: FILE:./prompt.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 16885.27it/s]\n",
      "100%|██████████| 81/81 [00:01<00:00, 52.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judgement accuracy of 10000 examples: 9499/10000 (94.99%)\n",
      "accuracy of 10000 examples: 9500/10000 (95.0%)\n",
      "{'carry0': 95.66787003610109, 'carry1': 93.84615384615384, 'carry2': 95.71469657750936, 'carry3': 95.41213063763608}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(94.99,\n",
       " 95.0,\n",
       " {'carry0': 95.66787003610109,\n",
       "  'carry1': 93.84615384615384,\n",
       "  'carry2': 95.71469657750936,\n",
       "  'carry3': 95.41213063763608})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from contextlib import nullcontext\n",
    "ctx = nullcontext()\n",
    "config={\n",
    "    'start': 'FILE:./prompt.txt',\n",
    "    'device': mydevice,\n",
    "    'temperature': 0.8\n",
    "}\n",
    "eval_addition_batch(config, model, ctx, encode, decode, judge=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating addition from: FILE:./extra_num_judge_prompt.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 16776.22it/s]\n",
      "100%|██████████| 83/83 [00:00<00:00, 176.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judgement accuracy of 10000 examples: 9415/10000 (94.15%)\n",
      "No judging probability of 10000 examples: 0/10000 (0.0%)\n",
      "{'carry0': 93.14079422382672, 'carry1': 93.7062937062937, 'carry2': 94.16163359217717, 'carry3': 96.65629860031105}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(94.15,\n",
       " 0.0,\n",
       " {'carry0': 93.14079422382672,\n",
       "  'carry1': 93.7062937062937,\n",
       "  'carry2': 94.16163359217717,\n",
       "  'carry3': 96.65629860031105})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from contextlib import nullcontext\n",
    "ctx = nullcontext()\n",
    "config={\n",
    "    'start': 'FILE:./extra_num_judge_prompt.txt',\n",
    "    'device': mydevice,\n",
    "}\n",
    "eval_judge_batch(config, model, ctx, encode, decode, max_new_tokens=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating addition from: FILE:./add_noise_judge_prompt.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 15096.86it/s]\n",
      "100%|██████████| 82/82 [00:00<00:00, 157.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judgement accuracy of 10000 examples: 6644/10000 (66.44%)\n",
      "No judging probability of 10000 examples: 0/10000 (0.0%)\n",
      "{'carry0': 62.15403128760529, 'carry1': 64.47552447552447, 'carry2': 68.47857348288755, 'carry3': 71.9284603421462}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(66.44,\n",
       " 0.0,\n",
       " {'carry0': 62.15403128760529,\n",
       "  'carry1': 64.47552447552447,\n",
       "  'carry2': 68.47857348288755,\n",
       "  'carry3': 71.9284603421462})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx = nullcontext()\n",
    "config={\n",
    "    'start': 'FILE:./add_noise_judge_prompt.txt',\n",
    "    'device': mydevice,\n",
    "}\n",
    "eval_judge_batch(config, model, ctx, encode, decode, max_new_tokens=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 新数据：20%positive + 80%negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "\n",
    "class create_test_data:\n",
    "    def __init__(self, non_overlap_data_path, num_test_samples=100) -> None:\n",
    "        if not os.path.exists(non_overlap_data_path):\n",
    "            raise ValueError(\"There is no nonoverlap data file\")\n",
    "        self.non_overlap_data_path = non_overlap_data_path\n",
    "        self.num_test_samples = num_test_samples\n",
    "        self.samples = None\n",
    "        # Open non-overlap data path to get a bunch of test samples\n",
    "        with open(self.non_overlap_data_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            random.shuffle(lines)\n",
    "            self.samples = lines[:self.num_test_samples]\n",
    "            \n",
    "        \n",
    "    def create_prompt(self):\n",
    "        \"\"\"\n",
    "        To create prompt data file like: 'Ta+b=' .\n",
    "        The output should be correct answer and judgement\n",
    "        \"\"\"\n",
    "        with open(f'prompt.txt', 'w') as f2:\n",
    "            for line in self.samples:\n",
    "                prompt = line.split('=')[0]+'=\\n'\n",
    "                f2.write(prompt)\n",
    "                    \n",
    "    def create_add_noise_judge_prompt(self):\n",
    "        \"\"\"\n",
    "        To create prompt data file like: 'a+b=c' and 'a+b=d', where 'd' means wrong answer\n",
    "        The output should be judgement\n",
    "        \"\"\"\n",
    "        with open('add_noise_judge_prompt.txt', 'w') as f3:\n",
    "            for line in self.samples:\n",
    "                # 取出表达式部分\n",
    "                prompt = line.split('T')[1].strip()\n",
    "                # prompt = prompt.split('=')[0].strip()\n",
    "                new_prompt = self.modify_result(prompt, random.randint(1, 9), 'noise_add')\n",
    "                f3.write(new_prompt + '?\\n')\n",
    "                \n",
    "    def create_extra_num_judge_prompt(self):\n",
    "        \"\"\"\n",
    "        To create prompt data file like: 'a+b=c' and 'a+b=cd', where 'd' means extra number\n",
    "        The output should be judgement\n",
    "        \"\"\"\n",
    "        with open('extra_num_judge_prompt.txt', 'w') as f4:\n",
    "            for line in self.samples:\n",
    "                # 取出表达式部分\n",
    "                prompt = line.split('T')[1].strip()\n",
    "                # prompt = prompt.split('=')[0].strip()\n",
    "                new_prompt = self.modify_result(prompt, random.randint(1, 9), 'extra_num')\n",
    "                f4.write(new_prompt + '?\\n')\n",
    "\n",
    "    \n",
    "    def modify_result(self, expression, addend, mode):\n",
    "        # 使用正则表达式提取表达式中的数字\n",
    "        match = re.match(r'(\\d+)\\+(\\d+)=(\\d+)', expression)\n",
    "    \n",
    "        if match:\n",
    "            # 提取数字并计算新的结果\n",
    "            num1 = int(match.group(1))\n",
    "            num2 = int(match.group(2))\n",
    "            result = int(match.group(3))\n",
    "            num_digit = len(match.group(3).strip())\n",
    "            if mode == 'extra_num':\n",
    "                if random.uniform(0,1)>0.5 and random.uniform(0,1)<0.75:\n",
    "                    extra = random.randint(1, 9)\n",
    "                    new_expression = f\"F{num1}+{num2}={result}{extra}\"\n",
    "                elif random.uniform(0,1)>0.75:\n",
    "                    extra = random.randint(1, 9)\n",
    "                    new_expression = f\"F{num1}+{num2}={extra}{result}\"\n",
    "                else:\n",
    "                    new_expression = f\"T{num1}+{num2}={result}\"\n",
    "            elif mode == 'noise_add':\n",
    "                if random.uniform(0,1)>0.5:\n",
    "                    # 决定添加错误的位置\n",
    "                    wrong_loc = random.randint(0, num_digit)\n",
    "        \n",
    "                    new_result = result + addend * (10**wrong_loc)\n",
    "\n",
    "                    # 构建新的表达式\n",
    "                    new_expression = f\"F{num1}+{num2}={new_result}\"\n",
    "                else:\n",
    "                    new_expression = f\"T{num1}+{num2}={result}\"\n",
    "            \n",
    "            else:\n",
    "                return \"Invalid modify pattern\"\n",
    "        \n",
    "            return new_expression\n",
    "        else:\n",
    "            return \"Invalid expression format\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_overlap_data_path = './data/get_data_with_label/train_3digit_bilabeled10000_nonoverlap.txt'\n",
    "num_examples = 10000\n",
    "new_creator = create_test_data(non_overlap_data_path, num_examples)\n",
    "new_creator.create_prompt()\n",
    "new_creator.create_extra_num_judge_prompt()\n",
    "new_creator.create_add_noise_judge_prompt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 10.63M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(16, 384)\n",
       "    (wpe): Embedding(256, 384)\n",
       "    (drop): Dropout(p=0.2, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=384, out_features=1152, bias=False)\n",
       "          (c_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.2, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=384, out_features=1536, bias=False)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=1536, out_features=384, bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=384, out_features=16, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import GPTConfig, GPT\n",
    "import torch\n",
    "\n",
    "# init from a model saved in a specific directory\n",
    "ckpt_path = 'ckpt_judge_acc_bilabel(0.2p).pt'\n",
    "checkpoint = torch.load(ckpt_path, map_location=mydevice)\n",
    "gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "model = GPT(gptconf)\n",
    "state_dict = checkpoint['model']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(mydevice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading meta from ./data/addition_bilabel/meta.pkl...\n"
     ]
    }
   ],
   "source": [
    "encode, decode = get_encode_decode('./data/addition_bilabel/meta.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating addition from: FILE:./prompt.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:03<00:00, 2800.92it/s]\n",
      "100%|██████████| 81/81 [00:04<00:00, 17.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judgement accuracy of 10000 examples: 9434/10000 (94.34%)\n",
      "accuracy of 10000 examples: 9434/10000 (94.34%)\n",
      "{'carry0': 95.57522123893806, 'carry1': 92.58426966292134, 'carry2': 95.56204379562044, 'carry3': 94.31818181818183}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(94.34,\n",
       " 94.34,\n",
       " {'carry0': 95.57522123893806,\n",
       "  'carry1': 92.58426966292134,\n",
       "  'carry2': 95.56204379562044,\n",
       "  'carry3': 94.31818181818183})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from contextlib import nullcontext\n",
    "ctx = nullcontext()\n",
    "config={\n",
    "    'start': 'FILE:./prompt.txt',\n",
    "    'device': mydevice,\n",
    "    'temperature': 0.8\n",
    "}\n",
    "eval_addition_batch(config, model, ctx, encode, decode, judge=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating addition from: FILE:./extra_num_judge_prompt.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:03<00:00, 3183.20it/s]\n",
      "100%|██████████| 83/83 [00:01<00:00, 73.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judgement accuracy of 10000 examples: 8865/10000 (88.64999999999999%)\n",
      "No judging probability of 10000 examples: 0/10000 (0.0%)\n",
      "{'carry0': 90.4424778761062, 'carry1': 87.80898876404495, 'carry2': 87.97080291970802, 'carry3': 90.37878787878788}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(88.64999999999999,\n",
       " 0.0,\n",
       " {'carry0': 90.4424778761062,\n",
       "  'carry1': 87.80898876404495,\n",
       "  'carry2': 87.97080291970802,\n",
       "  'carry3': 90.37878787878788})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from contextlib import nullcontext\n",
    "ctx = nullcontext()\n",
    "config={\n",
    "    'start': 'FILE:./extra_num_judge_prompt.txt',\n",
    "    'device': mydevice,\n",
    "}\n",
    "eval_judge_batch(config, model, ctx, encode, decode, max_new_tokens=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating addition from: FILE:./add_noise_judge_prompt.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:05<00:00, 1682.61it/s]\n",
      "100%|██████████| 82/82 [01:26<00:00,  1.06s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judgement accuracy of 10000 examples: 6695/10000 (66.95%)\n",
      "No judging probability of 10000 examples: 0/10000 (0.0%)\n",
      "{'carry0': 65.66371681415929, 'carry1': 66.01123595505618, 'carry2': 66.97810218978103, 'carry3': 71.06060606060606}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(66.95,\n",
       " 0.0,\n",
       " {'carry0': 65.66371681415929,\n",
       "  'carry1': 66.01123595505618,\n",
       "  'carry2': 66.97810218978103,\n",
       "  'carry3': 71.06060606060606})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx = nullcontext()\n",
    "config={\n",
    "    'start': 'FILE:./add_noise_judge_prompt.txt',\n",
    "    'device': mydevice,\n",
    "}\n",
    "eval_judge_batch(config, model, ctx, encode, decode, max_new_tokens=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
